{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADTDA5622 - Big Data Computing Foundations 2\n",
    "## Homework 4: Manipulating Data with PySpark SQL\n",
    "- Semester: Spring 2023\n",
    "- Instructor: Tom Bihari\n",
    "- Section: N/A\n",
    "- Student Name: Able Baker **(fill in)**\n",
    "- Student Email: baker.12345@osu.edu **(fill in)**\n",
    "- Student ID: 123456789 **(fill in)**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Section: Overview\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Objectives of This Assignment are:**\n",
    "1. To gain an understanding of how to set up a PySpark program.\n",
    "2. To gain experience using PySpark SQL to manipulate data to solve problems and answer questions.\n",
    "3. To get a small preview of Spark DataFrame functions we will use in the future.\n",
    "\n",
    "**Overview:**\n",
    "- The main purpose of this homework is to walk through a PySpark program.  The main tasks in this assignment (see below) are almost exactly the same as those in the previous SQLite assignment.  In this assignment, you will create SQL statements and run them in Spark SQL instead of SQLite.  Spoiler alert: The queries and results should be almost identical to those in HW2.  \n",
    "- Tasks 1 and 2 are done for you.  You can use them as examples when doing Task 3.\n",
    "- I also included a short example of how to use Spark DataFrame functions to accomplish the same results as the SQL queries.  We will cover this more in future modules.  For a comprehensive list of PySpark functions, see: https://sparkbyexamples.com/pyspark-tutorial/\n",
    "- NOTE: In order to create tables in SQL, we need to have Hive support enabled in the environment. I have not done that for now.  For the tasks below, I have used TEMP VIEWs instead of TABLES. (I didn't rename the term \"table\" (used in the SQLite assignment) to \"view\" below.)\n",
    "\n",
    "**Instructions:**\n",
    "- **Follow the instructions** in each section.\n",
    "- **Fill in** the **Conclusions** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hdRb4-6iBM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0dMPoL66gSq"
   },
   "source": [
    "# Section: Setup\n",
    "- Add any needed imports, helper functions, etc., here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8gmI3D97wDg",
    "outputId": "254e86a1-4dc3-4e9b-aeb4-56b7179e4339"
   },
   "outputs": [],
   "source": [
    "# USUALLY DO NOT NEED TO INSTALL PYSPARK IF YOU ARE ON \"Jupyter + Spark\" on OSC.\n",
    "# If you do not have PySpark installed on your machine, install using pip. (Uncomment next line and run it.)\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b1VnquUe562m",
    "outputId": "c4f8a500-591c-43f3-d418-881ee4b1305b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as SqlF\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the location of the shared data folder\n",
    "shared_data_directory = \"../shared_Sp23/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0dMPoL66gSq"
   },
   "source": [
    "# Section: Initialize Spark\n",
    "- Create a Spark session if it does not already exist.  This also sets up a Spark Context.\n",
    "- See: https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.Builder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyApp\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('MyApp') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext  # Get the context, so we have a short name for it if we need it.\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ElocRSXTQbH"
   },
   "source": [
    "# Section: Create DataFrames and Tables from CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFXgkCCzr906"
   },
   "source": [
    "For each table you want to load from a CSV file, do the following:\n",
    "1. Create a Spark dataframe from the csv file.\n",
    "2. Register the dataframe as a SQL table (give it a table name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- abbreviation: string (nullable = true)\n",
      "\n",
      "+----------+------------+\n",
      "|state     |abbreviation|\n",
      "+----------+------------+\n",
      "|Alabama   |AL          |\n",
      "|Alaska    |AK          |\n",
      "|Arizona   |AZ          |\n",
      "|Arkansas  |AR          |\n",
      "|California|CA          |\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abbrevs_df = spark.read.format('csv').load(shared_data_directory + \"state-abbrevs.csv\", header=True, inferSchema=True)\n",
    "abbrevs_df.printSchema()\n",
    "abbrevs_df.show(5, truncate=False)\n",
    "abbrevs_df.registerTempTable(\"abbrevs_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      "\n",
      "+----------+------+\n",
      "|state     |area  |\n",
      "+----------+------+\n",
      "|Alabama   |52423 |\n",
      "|Alaska    |656425|\n",
      "|Arizona   |114006|\n",
      "|Arkansas  |53182 |\n",
      "|California|163707|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "areas_df = spark.read.format('csv').load(shared_data_directory + \"state-areas.csv\", header=True, inferSchema=True)\n",
    "areas_df.printSchema()\n",
    "areas_df.show(5, truncate=False)\n",
    "areas_df.registerTempTable(\"areas_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- ages: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      "\n",
      "+-----+-------+----+----------+\n",
      "|state|ages   |year|population|\n",
      "+-----+-------+----+----------+\n",
      "|AL   |under18|2012|1117489.0 |\n",
      "|AL   |total  |2012|4817528.0 |\n",
      "|AL   |under18|2010|1130966.0 |\n",
      "|AL   |total  |2010|4785570.0 |\n",
      "|AL   |under18|2011|1125763.0 |\n",
      "+-----+-------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "populations_df = spark.read.format('csv').load(shared_data_directory + \"state-populations.csv\", header=True, inferSchema=True)\n",
    "populations_df.printSchema()\n",
    "populations_df.show(5, truncate=False)\n",
    "populations_df.registerTempTable(\"populations_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Task 1: Create a Table of Summarized State Populations\n",
    "- Get the min, average, and max total and under18 populations for each of the states over all years.\n",
    "- Call the table **summarized_state_populations**.\n",
    "- The table should have the following fields:\n",
    "  - state, ages, min_pop, avg_pop, max_pop\n",
    "  - It should be sorted by **state** (from A to Z), and within each state, by **under18** age range first, then **total** age range. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+------------------+---------+\n",
      "|state|   ages|  min_pop|           avg_pop|  max_pop|\n",
      "+-----+-------+---------+------------------+---------+\n",
      "|   AK|  total| 553290.0| 646204.8333333334| 735132.0|\n",
      "|   AK|under18| 177502.0|186672.95833333334| 192636.0|\n",
      "|   AL|  total|4050055.0|       4484527.875|4833722.0|\n",
      "+-----+-------+---------+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP VIEW IF EXISTS summarized_state_populations\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TEMP VIEW summarized_state_populations\n",
    "    AS SELECT state, ages, min(population) AS min_pop, avg(population) AS avg_pop, max(population) AS max_pop\n",
    "    FROM populations_table\n",
    "    GROUP BY state, ages\n",
    "    ORDER BY state ASC, ages ASC\n",
    "                \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * from summarized_state_populations\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+------------------+---------+\n",
      "|state|   ages|  min_pop|           avg_pop|  max_pop|\n",
      "+-----+-------+---------+------------------+---------+\n",
      "|   AK|  total| 553290.0| 646204.8333333334| 735132.0|\n",
      "|   AK|under18| 177502.0|186672.95833333334| 192636.0|\n",
      "|   AL|  total|4050055.0|       4484527.875|4833722.0|\n",
      "+-----+-------+---------+------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A LOOK AHEAD: We also can do this with PySpark DataFrames directly.\n",
    "\n",
    "summarized_state_populations_df = (populations_df\n",
    "    .groupBy(\"state\", \"ages\")\n",
    "    .agg(SqlF.min(\"population\").alias(\"min_pop\"),\n",
    "         SqlF.avg(\"population\").alias(\"avg_pop\"),\n",
    "         SqlF.max(\"population\").alias(\"max_pop\"))\n",
    "    .orderBy(\"state\", \"ages\"))\n",
    "    \n",
    "summarized_state_populations_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Task 2: Print the Top 3 States with the Largest Normalized Population Swings\n",
    "- Consider the difference between max and min \"total\" populations for each state.\n",
    "- Normalize by dividing by the average population for the state.\n",
    "- You can use the table from the previous query.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+---------+------------------+\n",
      "|state|  min_pop|  max_pop|pop_swing|    norm_pop_swing|\n",
      "+-----+---------+---------+---------+------------------+\n",
      "|   PR|3615086.0|      NaN|      NaN|               NaN|\n",
      "|   NV|1220695.0|2790136.0|1569441.0| 0.748474068327076|\n",
      "|   AZ|3684097.0|6626624.0|2942527.0|0.5557600371372102|\n",
      "+-----+---------+---------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT state, min_pop, max_pop, (max_pop - min_pop) AS pop_swing, ((max_pop - min_pop) / avg_pop) AS norm_pop_swing\n",
    "    FROM summarized_state_populations\n",
    "    WHERE ages = \"total\"\n",
    "    ORDER BY norm_pop_swing DESC\n",
    "    ''')\n",
    "\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Task 3: Create a Table of Population Densities\n",
    "- Follow each of the individual steps below.  There likely are shorter ways to do this task, but this way the steps are explicit.\n",
    "- I have given the shells of some of the queries, so you can \"fill in the blanks\".\n",
    "- I have included the expected outputs, so you can see if yours matches.\n",
    "- Name the table **population_densities_table**\n",
    "- The table should have the following fields:\n",
    "  - state_abbrev\n",
    "  - state_name\n",
    "  - area\n",
    "  - year\n",
    "  - population_total\n",
    "  - population_under_18\n",
    "  - population_18_and_over\n",
    "  - density_total\n",
    "  - density_under_18\n",
    "  - density_18_and_over\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a \"pop_and_name_table\"\n",
    "- Join the populations_table and abbrevs_table.\n",
    "- Table should have fields: state_abbrev, state_name, ages, year, population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: pop_and_name_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name     ages  year  population\n",
    "0           AL    Alabama  under18  2012   1117489.0\n",
    "1           AL    Alabama    total  2012   4817528.0\n",
    "2           AL    Alabama  under18  2010   1130966.0\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a \"pop_name_area_density_table\"\n",
    "- Join the pop_and_name_table and areas_table.\n",
    "- Table should have fields: state_abbrev, state_name, ages, year, population, area, density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: pop_name_area_density_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name     ages  year  population      area    density\n",
    "0           AK     Alaska    total  1990    553290.0  656425.0   0.842884\n",
    "1           AK     Alaska  under18  1990    177502.0  656425.0   0.270407\n",
    "2           AL    Alabama    total  1990   4050055.0   52423.0  77.257215\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create an \"under18_table\"\n",
    "- Select only the \"under18\" ages records from the pop_name_area_density_table.\n",
    "- Table should have fields: state_abbrev, state_name, year, population, area, density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: under18_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name  year  population      area    density\n",
    "0           AK     Alaska  1990    177502.0  656425.0   0.270407\n",
    "1           AL    Alabama  1990   1050041.0   52423.0  20.030159\n",
    "2           AR   Arkansas  1990    620933.0   53182.0  11.675623\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a \"total_table\"\n",
    "- Select only the \"total\" ages records from the pop_name_area_density_table.\n",
    "- Table should have fields: state_abbrev, state_name, year, population, area, density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: total_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name  year  population      area    density\n",
    "0           AK     Alaska  1990    553290.0  656425.0   0.842884\n",
    "1           AL    Alabama  1990   4050055.0   52423.0  77.257215\n",
    "2           AR   Arkansas  1990   2356586.0   53182.0  44.311722\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a \"total_and_under18_table\"\n",
    "- Join the under18_table and total_table.\n",
    "- Table should have fields: state_abbrev, state_name, year, area, total_pop, total_density, under18_pop, under18_density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: total_and_under18_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name  year      area  total_pop  total_density  \\\n",
    "0           AK     Alaska  1990  656425.0   553290.0       0.842884   \n",
    "1           AL    Alabama  1990   52423.0  4050055.0      77.257215   \n",
    "2           AR   Arkansas  1990   53182.0  2356586.0      44.311722   \n",
    "\n",
    "   under18_pop  under18_density  \n",
    "0     177502.0         0.270407  \n",
    "1    1050041.0        20.030159  \n",
    "2     620933.0        11.675623  \n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a \"total_under18_over18_table\"\n",
    "- Start with the total_and_under18_table and calculate columns for the over18_pop, over18_density.\n",
    "- Actually, we mean \"18 and over\", not \"over 18\".\n",
    "- Table should have fields: state_abbrev, state_name, year, area, total_pop, total_density, under18_pop, under18_density, over18_pop, over18_density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: total_under18_over18_table\n",
    "------------------------------------------\n",
    "  state_abbrev state_name  year      area  total_pop  total_density  \\\n",
    "0           AK     Alaska  1990  656425.0   553290.0       0.842884   \n",
    "1           AL    Alabama  1990   52423.0  4050055.0      77.257215   \n",
    "2           AR   Arkansas  1990   53182.0  2356586.0      44.311722   \n",
    "\n",
    "   under18_pop  under18_density  over18_pop  over18_density  \n",
    "0     177502.0         0.270407    375788.0        0.572477  \n",
    "1    1050041.0        20.030159   3000014.0       57.227057  \n",
    "2     620933.0        11.675623   1735653.0       32.636099  \n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the final population_densities_table\"\n",
    "- Start with the total_under18_over18_table.\n",
    "- Rename and reorder the columns to match the original request.\n",
    "- Sort by year and state abbreviation, ascending.\n",
    "- Table should have fields: state_abbrev, state_name, area, year, \n",
    "  - population_total, population_under_18, population_18_and_over, density_total, density_under_18, density_18_and_over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in, using the examples from Tasks 1 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXPECTED: TABLE OR VIEW: population_densities_table\n",
    "------------------------------------------\n",
    "  state_abbrev            state_name      area  year  population_total  \\\n",
    "0           AK                Alaska  656425.0  1990          553290.0   \n",
    "1           AL               Alabama   52423.0  1990         4050055.0   \n",
    "2           AR              Arkansas   53182.0  1990         2356586.0   \n",
    "3           AZ               Arizona  114006.0  1990         3684097.0   \n",
    "4           CA            California  163707.0  1990        29959515.0   \n",
    "5           CO              Colorado  104100.0  1990         3307618.0   \n",
    "6           CT           Connecticut    5544.0  1990         3291967.0   \n",
    "7           DC  District of Columbia      68.0  1990          605321.0   \n",
    "8           DE              Delaware    1954.0  1990          669567.0   \n",
    "9           FL               Florida   65758.0  1990        13033307.0   \n",
    "\n",
    "   population_under_18  population_18_and_over  density_total  \\\n",
    "0             177502.0                375788.0       0.842884   \n",
    "1            1050041.0               3000014.0      77.257215   \n",
    "2             620933.0               1735653.0      44.311722   \n",
    "3            1006040.0               2678057.0      32.314940   \n",
    "4            7980501.0              21979014.0     183.006927   \n",
    "5             881640.0               2425978.0      31.773468   \n",
    "6             752666.0               2539301.0     593.789141   \n",
    "7             112632.0                492689.0    8901.779412   \n",
    "8             165628.0                503939.0     342.664790   \n",
    "9            2988807.0              10044500.0     198.201086   \n",
    "\n",
    "   density_under_18  density_18_and_over  \n",
    "0          0.270407             0.572477  \n",
    "1         20.030159            57.227057  \n",
    "2         11.675623            32.636099  \n",
    "3          8.824448            23.490492  \n",
    "4         48.748685           134.258242  \n",
    "5          8.469164            23.304304  \n",
    "6        135.762266           458.026876  \n",
    "7       1656.352941          7245.426471  \n",
    "8         84.763562           257.901228  \n",
    "9         45.451610           152.749475  \n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cleanup and Exit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession. USUALLY NOT NEEDED.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Section: Conclusions\n",
    "- What did you learn from this exercise?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill this in."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2-Pyspark_Intro_SQL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
